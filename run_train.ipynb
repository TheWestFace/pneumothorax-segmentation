{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as albu \n",
    "import torch\n",
    "\n",
    "import importlib\n",
    "import functools\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dataset import *\n",
    "from train import Learning\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(\n",
    "    train_config, experiment_folder, pipeline_name, log_dir, fold_id,\n",
    "    train_dataloader, valid_dataloader, binarizer_fn, eval_fn):\n",
    "    \n",
    "    fold_logger = init_logger(log_dir, 'train_fold_{}.log'.format(fold_id))\n",
    "\n",
    "    best_checkpoint_folder = Path(experiment_folder, train_config['CHECKPOINTS']['BEST_FOLDER'])\n",
    "    best_checkpoint_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    checkpoints_history_folder = Path(\n",
    "        experiment_folder,\n",
    "        train_config['CHECKPOINTS']['FULL_FOLDER'],\n",
    "        'fold{}'.format(fold_id))\n",
    "    checkpoints_history_folder.mkdir(exist_ok=True, parents=True)\n",
    "    checkpoints_topk = train_config['CHECKPOINTS']['TOPK']\n",
    "\n",
    "    calculation_name = '{}_fold{}'.format(pipeline_name, fold_id)\n",
    "    \n",
    "    device = train_config['DEVICE']\n",
    "    \n",
    "    module = importlib.import_module(train_config['MODEL']['PY'])\n",
    "    model_class = getattr(module, train_config['MODEL']['CLASS'])\n",
    "    model = model_class(**train_config['MODEL']['ARGS'])\n",
    "\n",
    "    pretrained_model_config = train_config['MODEL'].get('PRETRAINED', False)\n",
    "    if pretrained_model_config: \n",
    "        loaded_pipeline_name = pretrained_model_config['PIPELINE_NAME']\n",
    "        pretrained_model_path = Path(\n",
    "            pretrained_model_config['PIPELINE_PATH'], \n",
    "            pretrained_model_config['CHECKPOINTS_FOLDER'],\n",
    "            '{}_fold{}.pth'.format(loaded_pipeline_name, fold_id)) \n",
    "        if pretrained_model_path.is_file():\n",
    "            model.load_state_dict(torch.load(pretrained_model_path))\n",
    "            fold_logger.info('load model from {}'.format(pretrained_model_path)) \n",
    "\n",
    "    if len(train_config['DEVICE_LIST']) > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    module = importlib.import_module(train_config['CRITERION']['PY'])\n",
    "    loss_class = getattr(module, train_config['CRITERION']['CLASS'])\n",
    "    loss_fn = loss_class(**train_config['CRITERION']['ARGS'])\n",
    "    \n",
    "    optimizer_class = getattr(torch.optim, train_config['OPTIMIZER']['CLASS'])\n",
    "    optimizer = optimizer_class(model.parameters(), **train_config['OPTIMIZER']['ARGS'])\n",
    "    scheduler_class = getattr(torch.optim.lr_scheduler, train_config['SCHEDULER']['CLASS'])\n",
    "    scheduler = scheduler_class(optimizer, **train_config['SCHEDULER']['ARGS'])\n",
    "    \n",
    "    n_epoches = train_config['EPOCHES']\n",
    "    grad_clip = train_config['GRADIENT_CLIPPING']\n",
    "    grad_accum = train_config['GRADIENT_ACCUMULATION_STEPS']\n",
    "    early_stopping = train_config['EARLY_STOPPING']\n",
    "    validation_frequency = train_config.get('VALIDATION_FREQUENCY', 1)\n",
    "    \n",
    "    freeze_model = train_config['MODEL']['FREEZE']\n",
    "    \n",
    "    Learning(\n",
    "        optimizer,\n",
    "        binarizer_fn,\n",
    "        loss_fn,\n",
    "        eval_fn,\n",
    "        device,\n",
    "        n_epoches,\n",
    "        scheduler,\n",
    "        freeze_model,\n",
    "        grad_clip,\n",
    "        grad_accum,\n",
    "        early_stopping,\n",
    "        validation_frequency,\n",
    "        calculation_name,\n",
    "        best_checkpoint_folder,\n",
    "        checkpoints_history_folder,\n",
    "        checkpoints_topk,\n",
    "        fold_logger\n",
    "    ).run_train(model,train_dataloader,valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = Path(\"experiments\")\n",
    "# Change this line to your own config file path\n",
    "config_folder = experiment_folder / \"configs\" / \"ResUNet_1024_test.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PIPELINE_NAME': 'resunet_1024', 'LOGGER_DIR': 'resunet/resunet_1024_test/log', 'SEED': 42, 'DEVICE': 'cuda', 'DEVICE_LIST': [0], 'WORKERS': 8, 'MODEL': {'PRETRAINED': {'PIPELINE_PATH': 'experiments/resunet', 'CHECKPOINTS_FOLDER': 'resunet_1024_3', 'PIPELINE_NAME': 'resunet_1024'}, 'FREEZE': True, 'PY': 'model', 'CLASS': 'ResUNet', 'ARGS': {'pretrained': False}}, 'FOLD': {'NUMBER': 5, 'USEFOLDS': [0, 1, 2, 3, 4], 'FILE': None}, 'USE_SAMPLER': True, 'NON_EMPTY_MASK_PROBA': 0.4, 'IMG_SIZE': 1024, 'CRITERION': {'PY': 'losses', 'CLASS': 'ComboLoss', 'ARGS': {'weights': {'bce': 1, 'dice': 1, 'focal': 1}}}, 'OPTIMIZER': {'CLASS': 'Adam', 'ARGS': {'lr': 1e-05, 'weight_decay': 5e-06}}, 'SCHEDULER': {'CLASS': 'CosineAnnealingLR', 'ARGS': {'T_max': 8, 'eta_min': 1e-07}}, 'BATCH_SIZE': 2, 'GRADIENT_ACCUMULATION_STEPS': 1, 'GRADIENT_CLIPPING': 0.1, 'EPOCHES': 1, 'EARLY_STOPPING': 5, 'CHECKPOINTS': {'FULL_FOLDER': 'resunet/resunet_1024_test', 'BEST_FOLDER': 'resunet/resunet_1024_test', 'TOPK': 3}, 'MASK_BINARIZER': {'PY': 'binarizer', 'CLASS': 'TripletMaskBinarization', 'ARGS': {'triplets': [[0.75, 600, 0.4], [0.75, 800, 0.4], [0.75, 1000, 0.4], [0.7, 2000, 0.35], [0.7, 2500, 0.35], [0.7, 3000, 0.35], [0.7, 2000, 0.3], [0.7, 2500, 0.3], [0.7, 3000, 0.3], [0.65, 2000, 0.3], [0.65, 2500, 0.3], [0.65, 3000, 0.3], [0.6, 2000, 0.3], [0.6, 2500, 0.3], [0.6, 3000, 0.3], [0.6, 2000, 0.25], [0.6, 2500, 0.25], [0.6, 3000, 0.25]]}}, 'EVALUATION_METRIC': {'PY': 'losses', 'CLASS': 'dice_metric', 'ARGS': {'per_image': True}}}\n"
     ]
    }
   ],
   "source": [
    "train_config = load_yaml(config_folder)\n",
    "\n",
    "log_dir = Path(experiment_folder, train_config['LOGGER_DIR'])\n",
    "log_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "main_logger = init_logger(log_dir, 'train_main.log')\n",
    "\n",
    "seed = train_config['SEED']\n",
    "init_seed(seed)\n",
    "main_logger.info(train_config)\n",
    "\n",
    "if \"DEVICE_LIST\" in train_config:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, train_config[\"DEVICE_LIST\"]))\n",
    "\n",
    "pipeline_name = train_config['PIPELINE_NAME']\n",
    "\n",
    "non_empty_mask_proba = train_config.get('NON_EMPTY_MASK_PROBA', 0)\n",
    "use_sampler = train_config['USE_SAMPLER']\n",
    "\n",
    "num_workers = train_config['WORKERS']\n",
    "batch_size = train_config['BATCH_SIZE']\n",
    "n_folds = train_config['FOLD']['NUMBER']\n",
    "\n",
    "usefolds = map(int, train_config['FOLD']['USEFOLDS'])\n",
    "# local_metric_fn, global_metric_fn = init_eval_fns(train_config)\n",
    "\n",
    "binarizer_module = importlib.import_module(train_config['MASK_BINARIZER']['PY'])\n",
    "binarizer_class = getattr(binarizer_module, train_config['MASK_BINARIZER']['CLASS'])\n",
    "binarizer_fn = binarizer_class(**train_config['MASK_BINARIZER']['ARGS'])\n",
    "\n",
    "eval_module = importlib.import_module(train_config['EVALUATION_METRIC']['PY'])\n",
    "eval_fn = getattr(eval_module, train_config['EVALUATION_METRIC']['CLASS'])\n",
    "eval_fn = functools.partial(eval_fn, **train_config['EVALUATION_METRIC']['ARGS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = train_config['IMG_SIZE']\n",
    "\n",
    "train_transform = albu.Compose([\n",
    "    albu.OneOf([\n",
    "        albu.RandomGamma(),\n",
    "        albu.RandomBrightnessContrast(),\n",
    "        ], p=0.5),\n",
    "    albu.OneOf([\n",
    "        albu.ElasticTransform(),\n",
    "        albu.GridDistortion(),\n",
    "        albu.OpticalDistortion(),\n",
    "        ], p=0.3),\n",
    "    albu.ShiftScaleRotate(scale_limit=(0, 0.1), rotate_limit=0),\n",
    "    albu.Resize(img_size, img_size, always_apply=True),\n",
    "    albu.Normalize()\n",
    "])\n",
    "\n",
    "valid_transform = albu.Compose([\n",
    "    albu.Resize(img_size, img_size, always_apply=True),\n",
    "    albu.Normalize()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of positive samples: 2669\n",
      "Amount of negative samples: 9378\n"
     ]
    }
   ],
   "source": [
    "positive_names_path=\"./data/2img_mask_npy/positive_imgs_names.npy\"\n",
    "negative_names_path=\"./data/2img_mask_npy/negative_imgs_names.npy\"\n",
    "positive_names = np.load(positive_names_path)\n",
    "negative_names = np.load(negative_names_path)\n",
    "\n",
    "print(f\"Amount of positive samples: {len(positive_names)}\")\n",
    "print(f\"Amount of negative samples: {len(negative_names)}\")\n",
    "\n",
    "train_names = np.concatenate((positive_names, negative_names))\n",
    "exist_labels = np.concatenate((np.ones(len(positive_names), dtype=np.uint8), np.zeros(len(negative_names), dtype=np.uint8)))\n",
    "\n",
    "fold_labels = np.load(\"./data/2img_mask_npy/fold_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training of 0 fold....\n",
      "Train dataset size: 9637\n",
      "Valid dataset size: 2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tommy\\AppData\\Local\\Temp\\ipykernel_19868\\2565189145.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from experiments\\resunet\\resunet_1024_3\\resunet_1024_fold0.pth\n",
      "0 epoch: \t start validation....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.84916 on (0.6, 3000, 0.25): 100%|██████████| 1205/1205 [04:30<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: \t Score: 0.84916\t Params: (0.6, 3000, 0.25)\n",
      "Start training of 1 fold....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 9637\n",
      "Valid dataset size: 2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tommy\\AppData\\Local\\Temp\\ipykernel_19868\\2565189145.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from experiments\\resunet\\resunet_1024_3\\resunet_1024_fold1.pth\n",
      "0 epoch: \t start validation....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.84517 on (0.6, 3000, 0.25): 100%|██████████| 1205/1205 [03:51<00:00,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: \t Score: 0.84517\t Params: (0.6, 3000, 0.25)\n",
      "Start training of 2 fold....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 9638\n",
      "Valid dataset size: 2409\n",
      "load model from experiments\\resunet\\resunet_1024_3\\resunet_1024_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tommy\\AppData\\Local\\Temp\\ipykernel_19868\\2565189145.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: \t start validation....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.83864 on (0.7, 3000, 0.3): 100%|██████████| 1205/1205 [03:49<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: \t Score: 0.83864\t Params: (0.7, 3000, 0.3)\n",
      "Start training of 3 fold....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 9638\n",
      "Valid dataset size: 2409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tommy\\AppData\\Local\\Temp\\ipykernel_19868\\2565189145.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from experiments\\resunet\\resunet_1024_3\\resunet_1024_fold3.pth\n",
      "0 epoch: \t start validation....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.84461 on (0.7, 2000, 0.3): 100%|██████████| 1205/1205 [03:51<00:00,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: \t Score: 0.84461\t Params: (0.7, 2000, 0.3)\n",
      "Start training of 4 fold....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 9638\n",
      "Valid dataset size: 2409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tommy\\AppData\\Local\\Temp\\ipykernel_19868\\2565189145.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from experiments\\resunet\\resunet_1024_3\\resunet_1024_fold4.pth\n",
      "0 epoch: \t start validation....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.84964 on (0.6, 3000, 0.25): 100%|██████████| 1205/1205 [04:00<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch: \t Score: 0.84964\t Params: (0.6, 3000, 0.25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for fold_id in usefolds:\n",
    "    main_logger.info('Start training of {} fold....'.format(fold_id))\n",
    "\n",
    "    train_dataset = PneumoDataset(\n",
    "        mode=\"train\",\n",
    "        fold_index=fold_id,\n",
    "        train_names=train_names,\n",
    "        fold_labels=fold_labels,\n",
    "        transform=valid_transform\n",
    "        )\n",
    "    train_sampler = PneumoSampler(\n",
    "        fold_index=fold_id,\n",
    "        demand_non_empty_proba=non_empty_mask_proba, \n",
    "        fold_labels=fold_labels, \n",
    "        exist_labels=exist_labels\n",
    "        )\n",
    "    if use_sampler:\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=train_dataset, \n",
    "            batch_size=batch_size,   \n",
    "            num_workers=num_workers, \n",
    "            sampler=train_sampler\n",
    "            )\n",
    "    else:\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=train_dataset, \n",
    "            batch_size=batch_size,   \n",
    "            num_workers=num_workers, \n",
    "            shuffle=True\n",
    "            )\n",
    "\n",
    "    valid_dataset = PneumoDataset(\n",
    "        mode=\"val\",\n",
    "        fold_index=fold_id,\n",
    "        train_names=train_names,\n",
    "        fold_labels=fold_labels,\n",
    "        transform=valid_transform\n",
    "        )\n",
    "    \n",
    "    valid_dataloader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers, \n",
    "        shuffle=False\n",
    "        )\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Valid dataset size: {len(valid_dataset)}\")\n",
    "\n",
    "    train_fold(\n",
    "        train_config, experiment_folder, pipeline_name, log_dir, fold_id,\n",
    "        train_dataloader, valid_dataloader, binarizer_fn, eval_fn\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
